#pragma once
#include "layer.h"

// ReLU Layer
class relu_layer : public layer {
public:
  relu_layer(unsigned level, std::vector<size_t> in_dims,
             std::vector<size_t> out_dims)
      : layer(level, in_dims, out_dims) {
    trainable_ = false;
  }
  std::string layer_type() const override { return std::string("relu"); }
  // ğ‘¦[ğ‘™] = max(0, ğ‘¦[ğ‘™âˆ’1])
  void forward_propagation(const tensor_t& in_data,
                           tensor_t& out_data) override {
    galois::do_all(
        galois::iterate((size_t)0, input_dims[0]),
        [&](const auto& i) {
          for (size_t j = 0; j < input_dims[1]; ++j)
            out_data[i][j] =
                std::max(in_data[i][j], (float_t)0) +
                negative_slope * std::min(in_data[i][j], (float_t)0);
        },
        galois::chunk_size<CHUNK_SIZE>(), galois::steal(),
        galois::loopname("relu_layer-fw"));
  }
  // ğœ•ğ¿ / ğœ•ğ‘¦[ğ‘™âˆ’1] = 0, ğ‘–ğ‘“ (ğ‘¦[ğ‘™] < 0)
  //              = ğœ•ğ¿ / ğœ•ğ‘¦ğ‘™ , ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
  void back_propagation(const tensor_t& in_data, const tensor_t& out_data,
                        tensor_t& out_grad, tensor_t& in_grad) override {}
};
